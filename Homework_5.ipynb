{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "data_path = './data/'\n",
    "reduced_graph_path = data_path + 'wiki-topcats-reduced.txt'\n",
    "page_names_path = data_path + 'wiki-topcats-page-names.txt'\n",
    "categories_path = data_path + 'wiki-topcats-categories.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulding the graph with networkx\n",
    "H = nx.read_edgelist(path=reduced_graph_path, delimiter=\"\\t\", create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645247\n",
      "461193\n",
      "True\n",
      "1.2436602635647606e-05\n",
      "11.471323285479182\n"
     ]
    }
   ],
   "source": [
    "print(len(H.edges.items()))\n",
    "print(len(H.nodes.items()))\n",
    "print(nx.is_directed(H))\n",
    "\n",
    "'''\n",
    "In mathematics, a dense graph is a graph in which the number\n",
    "of edges is close to the maximal number of edges.\n",
    "'''\n",
    "print(nx.density(H))\n",
    "\n",
    "# mean degree value\n",
    "mean = []\n",
    "\n",
    "for n in H.nodes.items():\n",
    "    mean.append(nx.degree(H, n[0]))\n",
    "\n",
    "print(sum(mean) / len(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can treat the wikipedia hyperliks graph as a subset of the webgraph, that is of course directed, since is possible (and common) that a page has a link to another and not vice versa, so the edge has to be directed.\n",
    "\n",
    "-We used networkx library's functions to find the number of nodes and edges:\n",
    "- Nodes are **N=2645247**\n",
    "- Edges are **L=461193**\n",
    "\n",
    "-Average node degree is equal to **11.47** so every wikipedia page has , on average, 11.5 hyperlinks from and to other wikipedia articles, since degree is the sum of in-degree and out-degree\n",
    "\n",
    "-In order to say if wikipedia hyperlinks graph is dense or not we hate to consider the number of edges compared to the number of all possible edges that is $N(N-1)$ so density of our graph is:\n",
    "\n",
    "$$ \\Delta = \\frac{L}{N(N-1)} = \\frac{461193}{6997329045762} = 0.000012 $$\n",
    "We can conclude that our graph is not dense, it is actually very sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess wiki-topcats-categories.txt file as we should consider categories that have 3500 articles, at least. We could do this using the famous bash tool \"awk\":\n",
    "\n",
    "```bash\n",
    "awk 'NF > 3501' data/wiki-topcats-categories.txt > data/wiki-topcats-categories__3500.txt\n",
    "```\n",
    "\n",
    "- NF is defined as 'Number of Fields' indicating the number of items inside a row (more generally, columns)\n",
    "- The value of 3501 is justified as we are considering the Category label (ex. Category:Telugu_actors; 581455 581966 582010 582033 582071 ... )\n",
    "\n",
    "Eventually, we count the lines of the resulting file (using another terminal tool called \"wc\")\n",
    "\n",
    "```bash\n",
    "wc -l data/wiki-topcats-categories__3500.txt\n",
    "```\n",
    "\n",
    "and we notice that the set of Categories has been narrowed down to **35** items from **17364**.\n",
    "\n",
    "```bash\n",
    "cut -f 1 -d ' ' data/wiki-topcats-categories__3500.txt\n",
    "```\n",
    "\n",
    "- \"f\" is the field position (or first columns, in this case)\n",
    "- \"d\" is the delimiter (space)\n",
    "\n",
    "These are the categories that satisfy our threshold\n",
    "\n",
    "```\n",
    "Category:English_footballers;\n",
    "Category:The_Football_League_players;\n",
    "Category:Association_football_forwards;\n",
    "Category:Association_football_goalkeepers;\n",
    "Category:Association_football_midfielders;\n",
    "Category:Association_football_defenders;\n",
    "Category:Living_people;\n",
    "Category:Year_of_birth_unknown;\n",
    "Category:Harvard_University_alumni;\n",
    "Category:Major_League_Baseball_pitchers;\n",
    "Category:Members_of_the_United_Kingdom_Parliament_for_English_constituencies;\n",
    "Category:Indian_films;\n",
    "Category:Year_of_death_missing;\n",
    "Category:English_cricketers;\n",
    "Category:Year_of_birth_missing_(living_people);\n",
    "Category:Rivers_of_Romania;\n",
    "Category:Main_Belt_asteroids;\n",
    "Category:Asteroids_named_for_people;\n",
    "Category:English-language_albums;\n",
    "Category:English_television_actors;\n",
    "Category:British_films;\n",
    "Category:English-language_films;\n",
    "Category:American_films;\n",
    "Category:Fellows_of_the_Royal_Society;\n",
    "Category:People_from_New_York_City;\n",
    "Category:American_Jews;\n",
    "Category:American_television_actors;\n",
    "Category:American_film_actors;\n",
    "Category:Debut_albums;\n",
    "Category:Black-and-white_films;\n",
    "Category:Year_of_birth_missing;\n",
    "Category:Place_of_birth_missing_(living_people);\n",
    "Category:Article_Feedback_Pilot;\n",
    "Category:American_military_personnel_of_World_War_II;\n",
    "Category:Windows_games;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to be a list of lists where each element represents a category (list of articles)\n",
    "# The highest is the value, the furthest a category is\n",
    "block_ranking = []\n",
    "\n",
    "# indexes\n",
    "categ2articles = {}\n",
    "articles2categ = {}\n",
    "\n",
    "# category used as C_0 (minimum set of items)\n",
    "min_categ_len = math.inf\n",
    "min_categ = (None, None)\n",
    "\n",
    "# with open(data_path + 'wiki-topcats-categories__3500.txt', 'r') as most_linked_categs:\n",
    "with open(categories_path, 'r') as most_linked_categs:\n",
    "    for i, line in enumerate(most_linked_categs):\n",
    "        line = line.rstrip('\\n')\n",
    "        line = line.split()\n",
    "        \n",
    "        # get name and articles for each category\n",
    "        categ_name, articles, articles_len = line[0].lstrip('Category:').rstrip(';'), line[1:], len(line[1:])\n",
    "        \n",
    "        # save the smaller categ within a certain length\n",
    "        if articles_len < min_categ_len and articles_len >= 350:\n",
    "            min_categ_len = articles_len\n",
    "            min_categ = (categ_name, articles)\n",
    "        \n",
    "        # discard the ones under 3500 items...\n",
    "        if articles_len <= 3500: continue\n",
    "        \n",
    "        # categ2articles\n",
    "        if not categ_name in categ2articles:\n",
    "            categ2articles[categ_name] = set(articles)\n",
    "        else:\n",
    "            categ2articles[categ_name].update(articles)\n",
    "        \n",
    "        # articles2categ\n",
    "        for a in articles:\n",
    "            if not a in articles2categ.items():\n",
    "                articles2categ[a] = set([categ_name])\n",
    "            else:\n",
    "                articles2categ[a].add(categ_name)\n",
    "\n",
    "# putting C_0 in place\n",
    "# we chose a small category to speed up code runtime\n",
    "block_ranking.insert(0, min_categ[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: 35\n",
      "Smallest category: UEFA_Euro_2000_players 350\n",
      "Category in position 0: English_footballers 9237\n",
      "Category in position 1: The_Football_League_players 9467\n"
     ]
    }
   ],
   "source": [
    "print('Categories:', len(categ2articles.items()))\n",
    "print('Smallest category:', min_categ[0], len(min_categ[1]))\n",
    "print('Category in position 0:', list(categ2articles.items())[0][0], len(list(categ2articles.items())[0][1]))\n",
    "print('Category in position 1:', list(categ2articles.items())[1][0], len(list(categ2articles.items())[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should compute the Shortest Path algorithm to get the nearest and the furthest Category from C0. To be fast enough to show some results we'll use networkx library shortest path algorithm, even though we wrote our dijkstra function (that's more a modified BFS as we're dealing with a not-weighted graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing UEFA_Euro_2000_players and The_Football_League_players\n",
      "processing UEFA_Euro_2000_players and Association_football_forwards\n",
      "processing UEFA_Euro_2000_players and Association_football_goalkeepers\n",
      "processing UEFA_Euro_2000_players and Association_football_midfielders\n",
      "processing UEFA_Euro_2000_players and Association_football_defenders\n",
      "processing UEFA_Euro_2000_players and Living_people\n",
      "processing UEFA_Euro_2000_players and Year_of_birth_unknown\n",
      "processing UEFA_Euro_2000_players and Harvard_University_alumni\n",
      "processing UEFA_Euro_2000_players and Major_League_Baseball_pitchers\n",
      "processing UEFA_Euro_2000_players and Members_of_the_United_Kingdom_Parliament_for_English_constituencies\n",
      "processing UEFA_Euro_2000_players and Indian_films\n",
      "processing UEFA_Euro_2000_players and Year_of_death_missing\n",
      "processing UEFA_Euro_2000_players and English_cricketers\n",
      "processing UEFA_Euro_2000_players and Year_of_birth_missing_(living_people)\n",
      "processing UEFA_Euro_2000_players and Rivers_of_Romania\n",
      "processing UEFA_Euro_2000_players and Main_Belt_asteroids\n",
      "processing UEFA_Euro_2000_players and Asteroids_named_for_people\n",
      "processing UEFA_Euro_2000_players and English-language_albums\n",
      "processing UEFA_Euro_2000_players and English_television_actors\n",
      "processing UEFA_Euro_2000_players and British_films\n",
      "processing UEFA_Euro_2000_players and English-language_films\n",
      "processing UEFA_Euro_2000_players and American_films\n",
      "processing UEFA_Euro_2000_players and Fellows_of_the_Royal_Society\n",
      "processing UEFA_Euro_2000_players and People_from_New_York_City\n",
      "processing UEFA_Euro_2000_players and American_Jews\n",
      "processing UEFA_Euro_2000_players and American_television_actors\n",
      "processing UEFA_Euro_2000_players and American_film_actors\n",
      "processing UEFA_Euro_2000_players and Debut_albums\n",
      "processing UEFA_Euro_2000_players and Black-and-white_films\n",
      "processing UEFA_Euro_2000_players and Year_of_birth_missing\n",
      "processing UEFA_Euro_2000_players and Place_of_birth_missing_(living_people)\n",
      "processing UEFA_Euro_2000_players and Article_Feedback_Pilot\n",
      "processing UEFA_Euro_2000_players and American_military_personnel_of_World_War_II\n",
      "processing UEFA_Euro_2000_players and Windows_games\n",
      "[('UEFA_Euro_2000_players', 0), ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 7.0), ('English-language_albums', 7.0), ('English_television_actors', 7.0), ('British_films', 7.0), ('English-language_films', 7.0), ('American_films', 7.0), ('American_Jews', 7.0), ('American_television_actors', 7.0), ('American_film_actors', 7.0), ('Debut_albums', 7.0), ('Black-and-white_films', 7.0), ('Article_Feedback_Pilot', 7.0), ('The_Football_League_players', 8.0), ('Harvard_University_alumni', 8.0), ('Indian_films', 8.0), ('Rivers_of_Romania', 8.0), ('Fellows_of_the_Royal_Society', 8.0), ('People_from_New_York_City', 8.0), ('Place_of_birth_missing_(living_people)', 8.0), ('American_military_personnel_of_World_War_II', 8.0), ('Windows_games', 8.0), ('Association_football_defenders', 9.0), ('Living_people', 9.0), ('English_cricketers', 12.0), ('Association_football_forwards', inf), ('Association_football_goalkeepers', inf), ('Association_football_midfielders', inf), ('Year_of_birth_unknown', inf), ('Major_League_Baseball_pitchers', inf), ('Year_of_death_missing', inf), ('Year_of_birth_missing_(living_people)', inf), ('Main_Belt_asteroids', inf), ('Asteroids_named_for_people', inf), ('Year_of_birth_missing', inf)]\n"
     ]
    }
   ],
   "source": [
    "# whole set of categories (except the min_categ)\n",
    "categ_list = list(categ2articles.items())\n",
    "\n",
    "# min_categ (used as C_0) taken into account to speed up the runtime\n",
    "c_zero = min_categ\n",
    "\n",
    "# shortest path for each category\n",
    "distances = [(min_categ[0], 0)] * len(categ_list)\n",
    "\n",
    "# save up some time storing and retrieving\n",
    "# already computed paths\n",
    "distances_memoization = {}\n",
    "\n",
    "# stats\n",
    "skipped_memoization = 0\n",
    "skipped_no_path = 0\n",
    "skipped_no_nodes_in_H = 0\n",
    "total = 0\n",
    "\n",
    "# shortest path\n",
    "def compute_sp(c0, ci):\n",
    "    \n",
    "    # use global variables for stats\n",
    "    global total, skipped_memoization, skipped_no_path, skipped_no_nodes_in_H\n",
    "    \n",
    "    # distance values\n",
    "    sp_values = []\n",
    "    \n",
    "    # for each node inside the input category C_0\n",
    "    for c_x in c0:\n",
    "        \n",
    "        # for each node inside the i-st category C_i\n",
    "        for c_y in ci:\n",
    "            total += 1\n",
    "            \n",
    "            # key used for memoization\n",
    "            key = c_x + '_' + c_y\n",
    "            \n",
    "            # if we've already computed it, use it!\n",
    "            if key in distances_memoization:\n",
    "                \n",
    "                # save value for later use\n",
    "                sp_values.append(distances_memoization[key])\n",
    "                skipped_memoization += 1\n",
    "                continue\n",
    "            \n",
    "            # skip in case of neither c_x nor c_y are contained inside the graph\n",
    "            if not c_x in H or not c_y in H:\n",
    "                skipped_no_nodes_in_H += 1\n",
    "                continue\n",
    "            \n",
    "            # if there's not any path, set infinite as value \n",
    "            if not nx.has_path(H, c_x, c_y):\n",
    "                sp_values.append(math.inf)\n",
    "                \n",
    "                # save for later use\n",
    "                distances_memoization[key] = math.inf\n",
    "                skipped_no_path += 1\n",
    "                continue\n",
    "            \n",
    "            # we defined our own shortest function but to be fast enough\n",
    "            # we're using the one from the library\n",
    "            shortest_path = nx.shortest_path(H, c_x, c_y)\n",
    "            shortest_path_len = len(shortest_path)\n",
    "            \n",
    "            # append the path length value\n",
    "            sp_values.append(shortest_path_len)\n",
    "            \n",
    "            # save for later use\n",
    "            distances_memoization[key] = shortest_path_len\n",
    "    \n",
    "    # return the median if the length is sufficient\n",
    "    return statistics.median(sorted(sp_values)) if len(sp_values) > 0 else sp_values\n",
    "\n",
    "# Filling and sorting our block_ranking\n",
    "for i in range(1, len(categ_list)):\n",
    "    \n",
    "    # i-st category \n",
    "    c_ist = categ_list[i]\n",
    "    \n",
    "    # info\n",
    "    print('processing', c_zero[0], 'and', c_ist[0])\n",
    "    \n",
    "    # computing distances using a batch of nodes to be fast enough\n",
    "    # remove the slicing to work on the full set of nodes related to the c-st category\n",
    "    distances[i] = (c_ist[0], compute_sp(list(c_zero[1]), list(c_ist[1])[:50]))\n",
    "\n",
    "# show our block ranking computed on a subset of data\n",
    "print(sorted(distances, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595000 51340 157598 90100\n"
     ]
    }
   ],
   "source": [
    "# info about computed path, skipped, retrieved from a dict to be re-used (memoization) \n",
    "print(total, skipped_memoization, skipped_no_path, skipped_no_nodes_in_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['27108', '27301', '27305', '27377', '27383', '27402', '27450', '27459', '27561', '27562', '27566', '27607', '27994', '28057', '73449', '73491', '73642', '73643', '73664', '73666', '73810', '73811', '74001', '74083', '76809', '76810', '76811', '76906', '76910', '78971', '78978', '80393', '80395', '80396', '80397', '80709', '81320', '81321', '81463', '81609', '81826', '81827', '81828', '81829', '81854', '81858', '81867', '81885', '81915', '81926', '81942', '82051', '82058', '82071', '82091', '82317', '82321', '82366', '82368', '82369', '82393', '82476', '82477', '82537', '82735', '82738', '82740', '82742', '82745', '82970', '83047', '83472', '83500', '83507', '83512', '83513', '83514', '83516', '83526', '83527', '83528', '83954', '83958', '83960', '83984', '83985', '83988', '84176', '84178', '84279', '84328', '84384', '84385', '84391', '84392', '84393', '84845', '84848', '84850', '84879', '84892', '84893', '84957', '84958', '84959', '84966', '85008', '85214', '85221', '85414', '85422', '85609', '85701', '85706', '85708', '85974', '87487', '87488', '87493', '87496', '87497', '87498', '87502', '87504', '87506', '87566', '87590', '87591', '87596', '87605', '87667', '87678', '87682', '87688', '87695', '87697', '87703', '87713', '87718', '87728', '87732', '88756', '88797', '88800', '88801', '88806', '88808', '88813', '88815', '88836', '88846', '88858', '88859', '88861', '88862', '88870', '88873', '88875', '88876', '88878', '88883', '88887', '88888', '88896', '88906', '88909', '88919', '88925', '88933', '88941', '88945', '88948', '88949', '88950', '88973', '88975', '89294', '89299', '89613', '89616', '89617', '89633', '89698', '89700', '89704', '89712', '89714', '89723', '89869', '89871', '89872', '89889', '90048', '90053', '90123', '90128', '90129', '90141', '90165', '90168', '96907', '105732', '105938', '365677', '365678', '365680', '365711', '365725', '365737', '365745', '365783', '365784', '365785', '365786', '365856', '365872', '365904', '365905', '365924', '366108', '366149', '734341', '734382', '734383', '734498', '734501', '734523', '734567', '734618', '734619', '734625', '734631', '734634', '734644', '734694', '734695', '734752', '734757', '735737', '736078', '736253', '736427', '736575', '736577', '736596', '736618', '736672', '736831', '736960', '737074', '737121', '737124', '737648', '737853', '737895', '737906', '737908', '737917', '737930', '737931', '737938', '738029', '738103', '893767', '895672', '895673', '895705', '895836', '895881', '895923', '895925', '895934', '895935', '895936', '896030', '896056', '896062', '896301', '896573', '896872', '1358575', '1358581', '1358629', '1358669', '1358679', '1358682', '1358701', '1358741', '1358775', '1358776', '1358781', '1358784', '1358786', '1358823', '1358939', '1359033', '1359380', '1359908', '1360509', '1361639', '1625866', '1626244', '1626245', '1626258', '1626260', '1626336', '1626352', '1626359', '1626416', '1626451', '1626456', '1626466', '1638687', '1638761', '1638958', '1639022', '1639023', '1639024', '1639025', '1639152', '1639154', '1639162', '1693034', '1693040', '1693043', '1693204', '1693257', '1693261', '1693276', '1693315', '1693332', '1712998', '1713140', '1713174', '1713175', '1713176', '1713178', '1713179', '1713180', '1713182', '1713183', '1713184', '1713185', '1713186', '1713187', '1713188', '1713189', '1713190', '1713191', '1713202']\n"
     ]
    }
   ],
   "source": [
    "# We add our min_categ (350 items) to categ2articles since it has been excluded from\n",
    "# the creation of this data structure.\n",
    "# This category is needed in order to compute the score.\n",
    "categ2articles[min_categ[0]] = min_categ[1]\n",
    "print(categ2articles[min_categ[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English_footballers 50\n",
      "The_Football_League_players 50\n",
      "Association_football_forwards 50\n",
      "Association_football_goalkeepers 50\n",
      "Association_football_midfielders 50\n",
      "Association_football_defenders 50\n",
      "Living_people 50\n",
      "Year_of_birth_unknown 50\n",
      "Harvard_University_alumni 50\n",
      "Major_League_Baseball_pitchers 50\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 50\n",
      "Indian_films 50\n",
      "Year_of_death_missing 50\n",
      "English_cricketers 50\n",
      "Year_of_birth_missing_(living_people) 50\n",
      "Rivers_of_Romania 50\n",
      "Main_Belt_asteroids 50\n",
      "Asteroids_named_for_people 50\n",
      "English-language_albums 50\n",
      "English_television_actors 50\n",
      "British_films 50\n",
      "English-language_films 50\n",
      "American_films 50\n",
      "Fellows_of_the_Royal_Society 50\n",
      "People_from_New_York_City 50\n",
      "American_Jews 50\n",
      "American_television_actors 50\n",
      "American_film_actors 50\n",
      "Debut_albums 50\n",
      "Black-and-white_films 50\n",
      "Year_of_birth_missing 50\n",
      "Place_of_birth_missing_(living_people) 50\n",
      "Article_Feedback_Pilot 50\n",
      "American_military_personnel_of_World_War_II 50\n",
      "Windows_games 50\n",
      "UEFA_Euro_2000_players 50\n"
     ]
    }
   ],
   "source": [
    "# we compute the score using only 50 nodes\n",
    "# to be faster but less accurate. We decided to proceed using a subset.\n",
    "# Running the code without slicing will take hours but it'll work.\n",
    "score = {n: 0 for n in H.nodes()} \n",
    "visited, sub_H_nodes = set(), list()\n",
    "\n",
    "# loop over category\n",
    "for cat in categ2articles.keys():\n",
    "    \n",
    "    # taking a batch of nodes to be fast enough\n",
    "    # remove the slicing to work on the full set of nodes \n",
    "    cat_nodes = list(categ2articles[cat])[:50]\n",
    "    \n",
    "    # info about the processed categories and their batches of nodes\n",
    "    print(cat, len(cat_nodes))\n",
    "    \n",
    "    # induced subgraph through networkx\n",
    "    sub_H_nodes.extend(cat_nodes)\n",
    "    I = H.subgraph(sub_H_nodes)\n",
    "    \n",
    "    for n in cat_nodes:\n",
    "        \n",
    "        # skip node (related to the category) if not contained inside the induced subgraph\n",
    "        if not n in I: continue\n",
    "        \n",
    "        # for each predecessor (the nodes that lie behind of n)\n",
    "        for predecessor in list(I.predecessors(n)):\n",
    "            \n",
    "            # compute the score\n",
    "            score[n] += score[predecessor] if predecessor in visited else 1\n",
    "    \n",
    "    # set all nodes in this category as visited\n",
    "    for n in cat_nodes:\n",
    "        visited.add(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page-names\n",
    "# this code help to debug and to show results (basically, it links a node id to pagename)\n",
    "page2names = {}\n",
    "names2page = {}\n",
    "\n",
    "# open page-name file\n",
    "with open(page_names_path, 'r') as page_names:\n",
    "    for line in page_names:\n",
    "        node_id, node_name = line.rstrip('\\n').split(' ', 1)\n",
    "        \n",
    "        # page2names\n",
    "        page2names[node_id] = node_name\n",
    "        \n",
    "        # names2page\n",
    "        names2page[node_name] = node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hung Huang\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# info\n",
    "print(page2names['52'])\n",
    "print(names2page['Hung Huang'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did in step 1, 2 and 3 was to create a induced subgraph from each category (sequentially) and to score their nodes. If the predecessor of the node came from the same category, the assigned score is one. Otherwise, if it came from precedent categories, they heir their score. The process ends when a massive graph has been create after processing all categories, eventually, obtaining the same graph as the originalmo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FIRST 50 ---\n",
      "Steve McManaman\n",
      "Robbie Fowler\n",
      "Tony Adams (footballer)\n",
      "Steven Gerrard\n",
      "Zinedine Zidane\n",
      "Nick Barmby\n",
      "Henning Berg\n",
      "Roar Strand\n",
      "Paul Ince\n",
      "Vegard Heggem\n",
      "PlayStation 3\n",
      "Progressivism\n",
      "Gheorghe Hagi\n",
      "Bogdan Stelea\n",
      "Dorinel Munteanu\n",
      "Burt Lancaster\n",
      "Richard Avedon\n",
      "Warcraft III: Reign of Chaos\n",
      "Nigel Martyn\n",
      "Dan Eggen\n",
      "Martin Keown\n",
      "Peter Schmeichel\n",
      "Richard Wright (footballer)\n",
      "Gareth Barry\n",
      "Sol Campbell\n",
      "The Noah's Ark Trap\n",
      "From the Devil to a Stranger\n",
      "Fabio Coltorti\n",
      "Cyril Washbrook\n",
      "Graham Atkinson (cricketer)\n",
      "Frank Rost\n",
      "Hung Huang\n",
      "Anna Wintour\n",
      "Chen Kaige\n",
      "Oprah Winfrey\n",
      "Richard Cytowic\n",
      "Oliver Sacks\n",
      "Dmitri Nabokov\n",
      "United States\n",
      "David Eagleman\n",
      "Jay Hambidge\n",
      "Ronald Reagan\n",
      "James Brady\n",
      "American University\n",
      "Tim Tebow\n",
      "Cretien van Campen\n",
      "Philosophy\n",
      "James Wannerton\n",
      "CBS\n",
      "Ty Segall\n",
      "\n",
      "--- LAST 50 ---\n",
      "Sonny Caldinez\n",
      "Lesley Scott\n",
      "Keith Temple\n",
      "Mat Irvine\n",
      "Robbie Stamp\n",
      "Tim Child\n",
      "Rosala Mera\n",
      "Amancio Ortega Gaona\n",
      "Pablo Isla\n",
      "Lester Estelle II\n",
      "Morning's Wrath\n",
      "John L. Hines, Jr.\n",
      "This Will Destroy You (album)\n",
      "Khalid Abdel Nasser\n",
      "Ian Bird (software developer)\n",
      "Walid Husayin\n",
      "Vitali Seletskiy\n",
      "Igor Reshetnev\n",
      "Yegor Lunev\n",
      "Vyacheslav Bazanov\n",
      "Nikolai Matyukhin\n",
      "Maksim Krychanov\n",
      "Sergei Grigoryev\n",
      "Aleksei Vladimirovich Zakharov\n",
      "Aleksei Sergeyevich Zakharov\n",
      "Yevgeni Yevgenyevich Chernyshov\n",
      "Sergei Fyodorovich Semyonov\n",
      "Aleksei Yeryomin\n",
      "Aleksei Sergeyevich Perminov\n",
      "Nikolai Vasilyevich Zuyev\n",
      "Yevgeni Vladimirovich Korolyov\n",
      "Aleksei Alekseyevtsev\n",
      "Oleg Korolyov\n",
      "Rosemary Mahoney\n",
      "Grard Mulliez\n",
      "Nobutada Saji\n",
      "Tadashi Yanai\n",
      "Masatoshi Ito\n",
      "Fukuzo Iwasaki\n",
      "Kazuo Inamori\n",
      "Hajime Satomi\n",
      "John P. Costas (engineer)\n",
      "Aleksei Andreyevich Alekseyev\n",
      "Maksim Nikolayevich Lepskiy\n",
      "George R. Fischer\n",
      "Sergey Alexeyevich Kiselyov\n",
      "Bobby Kerr (footballer)\n",
      "Noadiah Russell (Yale founder)\n",
      "Peter Grummitt\n",
      "Catherine Dubosc\n"
     ]
    }
   ],
   "source": [
    "score_sorted = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('---', 'FIRST 50' ,'---')\n",
    "\n",
    "for n_k, n_v in score_sorted[:50]:\n",
    "    print(page2names[n_k])\n",
    "\n",
    "print('\\n---', 'LAST 50' ,'---')\n",
    "\n",
    "for n_k, n_v in score_sorted[-50:]:\n",
    "    print(page2names[n_k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
